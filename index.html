<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Lip Movement and Speech Recognition</title>
  <style>
    #videoElement {
      width: 100%;
      height: auto;
      border: 2px solid black;
    }
    #canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: auto;
    }
    #speechLog {
      margin-top: 20px;
      font-family: monospace;
    }
  </style>
</head>
<body>

<video id="videoElement" autoplay></video>
<canvas id="canvas"></canvas>
<div id="speechLog">
  <h2>Speech Recognition Log:</h2>
  <p id="speechText">Waiting for speech...</p>
</div>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>

<script>
  // Get the video element and canvas
  const videoElement = document.getElementById('videoElement');
  const canvas = document.getElementById('canvas');
  const context = canvas.getContext('2d');
  const speechText = document.getElementById('speechText');

  let stream;
  let faceModel;
  let speechRecognition;
  let isTalking = false;

  // Access webcam
  async function startWebcam() {
    stream = await navigator.mediaDevices.getUserMedia({
      video: true
    });
    videoElement.srcObject = stream;
    videoElement.onloadedmetadata = () => {
      canvas.width = videoElement.videoWidth;
      canvas.height = videoElement.videoHeight;
    };
  }

  // Load TensorFlow.js face landmark model
  async function loadFaceModel() {
    faceModel = await faceLandmarksDetection.load(faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);
    detectFaceLandmarks();
  }

  // Detect face landmarks and lip frame
  async function detectFaceLandmarks() {
    const predictions = await faceModel.estimateFaces({
      input: videoElement
    });

    context.clearRect(0, 0, canvas.width, canvas.height);
    if (predictions.length > 0) {
      // Extract lip landmarks (mouth points 61 to 80 in the model)
      const mouth = predictions[0].annotations.mouth;

      // Draw the lip frame on the canvas
      context.beginPath();
      context.moveTo(mouth[0][0], mouth[0][1]);
      for (let i = 1; i < mouth.length; i++) {
        context.lineTo(mouth[i][0], mouth[i][1]);
      }
      context.closePath();
      context.lineWidth = 2;
      context.strokeStyle = "green";
      context.stroke();
    }

    // Recursively call detectFaceLandmarks for continuous detection
    requestAnimationFrame(detectFaceLandmarks);
  }

  // Setup speech recognition using Web Speech API
  function setupSpeechRecognition() {
    if (!('SpeechRecognition' in window || 'webkitSpeechRecognition' in window)) {
      speechText.innerText = "Speech Recognition not supported.";
      return;
    }

    const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.continuous = true;
    recognition.interimResults = true;

    recognition.onstart = () => {
      speechText.innerText = "Listening for speech...";
    };

    recognition.onresult = (event) => {
      const last = event.results.length - 1;
      const transcript = event.results[last][0].transcript;
      const isFinal = event.results[last].isFinal;

      if (isFinal) {
        speechText.innerText = `You said: "${transcript}"`;
      }
    };

    recognition.onerror = (event) => {
      speechText.innerText = `Error: ${event.error}`;
    };

    recognition.start();
  }

  // Initialize everything
  async function initialize() {
    await startWebcam();
    await loadFaceModel();
    setupSpeechRecognition();
  }

  // Start the process
  initialize();
</script>

</body>
</html>
