<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Lip Movement Masking</title>
  <style>
    #videoElement {
      width: 100%;
      height: auto;
      border: 2px solid black;
    }
    #canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: auto;
    }
    #speechLog {
      margin-top: 20px;
      font-family: monospace;
    }
  </style>
</head>
<body>

<video id="videoElement" autoplay></video>
<canvas id="canvas"></canvas>
<div id="speechLog">
  <h2>Speech Recognition Log:</h2>
  <p id="speechText">Waiting for speech...</p>
</div>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>

<script>
  // Get the video element and canvas
  const videoElement = document.getElementById('videoElement');
  const canvas = document.getElementById('canvas');
  const context = canvas.getContext('2d');
  const speechText = document.getElementById('speechText');

  let stream;
  let faceModel;
  let speechRecognition;
  let isTalking = false;

  // Access webcam
  async function startWebcam() {
    stream = await navigator.mediaDevices.getUserMedia({
      video: true
    });
    videoElement.srcObject = stream;
    videoElement.onloadedmetadata = () => {
      canvas.width = videoElement.videoWidth;
      canvas.height = videoElement.videoHeight;
    };
  }

  // Load TensorFlow.js face landmark model
  async function loadFaceModel() {
    faceModel = await faceLandmarksDetection.load(faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);
    detectFaceLandmarks();
  }

  // Detect face landmarks and apply lip masking
  async function detectFaceLandmarks() {
    const predictions = await faceModel.estimateFaces({
      input: videoElement
    });

    context.clearRect(0, 0, canvas.width, canvas.height);
    if (predictions.length > 0) {
      // Extract lip landmarks (mouth points 61 to 80 in the model)
      const mouth = predictions[0].annotations.mouth;

      // Create a new imageData object to draw the mask
      let imageData = context.createImageData(canvas.width, canvas.height);
      let pixels = imageData.data;

      // Mask everything except the lip region
      for (let i = 0; i < pixels.length; i += 4) {
        // Get the current pixel's position
        const x = (i / 4) % canvas.width;
        const y = Math.floor(i / 4 / canvas.width);

        // Check if the pixel is inside the mouth region (using simple polygon check)
        if (isInsideMouth(x, y, mouth)) {
          // If inside the mouth, retain the original video pixel (copy from video)
          const videoPixel = getVideoPixel(x, y);
          pixels[i] = videoPixel.r;
          pixels[i + 1] = videoPixel.g;
          pixels[i + 2] = videoPixel.b;
          pixels[i + 3] = 255; // Set alpha to fully visible
        } else {
          // Else make the pixel transparent
          pixels[i + 3] = 0; // Transparent pixel
        }
      }

      // Put the manipulated image data onto the canvas
      context.putImageData(imageData, 0, 0);
    }

    // Recursively call detectFaceLandmarks for continuous detection
    requestAnimationFrame(detectFaceLandmarks);
  }

  // Check if a point is inside the mouth polygon (simple ray-casting method)
  function isInsideMouth(x, y, mouthPoints) {
    let inside = false;
    for (let i = 0, j = mouthPoints.length - 1; i < mouthPoints.length; j = i++) {
      const xi = mouthPoints[i][0], yi = mouthPoints[i][1];
      const xj = mouthPoints[j][0], yj = mouthPoints[j][1];

      const intersect = ((yi > y) !== (yj > y)) && (x < (xj - xi) * (y - yi) / (yj - yi) + xi);
      if (intersect) inside = !inside;
    }
    return inside;
  }

  // Get the corresponding pixel from the video stream
  function getVideoPixel(x, y) {
    const videoFrame = context.getImageData(x, y, 1, 1);
    const pixel = videoFrame.data;
    return {
      r: pixel[0],
      g: pixel[1],
      b: pixel[2]
    };
  }

  // Setup speech recognition using Web Speech API
  function setupSpeechRecognition() {
    if (!('SpeechRecognition' in window || 'webkitSpeechRecognition' in window)) {
      speechText.innerText = "Speech Recognition not supported.";
      return;
    }

    const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.continuous = true;
    recognition.interimResults = true;

    recognition.onstart = () => {
      speechText.innerText = "Listening for speech...";
    };

    recognition.onresult = (event) => {
      const last = event.results.length - 1;
      const transcript = event.results[last][0].transcript;
      const isFinal = event.results[last].isFinal;

      if (isFinal) {
        speechText.innerText = `You said: "${transcript}"`;
      }
    };

    recognition.onerror = (event) => {
      speechText.innerText = `Error: ${event.error}`;
    };

    recognition.start();
  }

  // Initialize everything
  async function initialize() {
    await startWebcam();
    await loadFaceModel();
    setupSpeechRecognition();
  }

  // Start the process
  initialize();
</script>

</body>
</html>
